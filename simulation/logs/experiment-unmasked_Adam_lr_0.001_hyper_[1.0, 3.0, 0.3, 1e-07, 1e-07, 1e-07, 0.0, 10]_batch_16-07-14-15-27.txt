Starting unmasked_Adam_lr_0.001_hyper_[1.0, 3.0, 0.3, 1e-07, 1e-07, 1e-07, 0.0, 10]_batch_16-07-14-15-27 experiment
Use unmasked matrix as linear system.
Use Adam as optimaizer, with learning rate 0.001.
The structure of encoder is [3, 32, 64, 8], and decoder is [11, 128, 64, 32, 3].
Hyper parameter used are [1.0, 3.0, 0.3, 1e-07, 1e-07, 1e-07, 0.0, 10]
The size of batch is 16
=> no checkpoint found at './weight/unmasked_Adam_lr_0.001_hyper_[1.0, 3.0, 0.3, 1e-07, 1e-07, 1e-07, 0.0, 10]_batch_16-07-14-15-27.pt'
old model loaded.
val_loss: 2.015
val_loss: 0.546
model saved.
val_loss: 0.396
model saved.
val_loss: 0.267
model saved.
val_loss: 0.219
model saved.
val_loss: 0.245
val_loss: 0.247
val_loss: 0.232
val_loss: 0.225
val_loss: 0.245
val_loss: 0.203
model saved.
val_loss: 0.178
model saved.
val_loss: 0.219
val_loss: 0.182
val_loss: 0.211
val_loss: 0.196
val_loss: 0.223
val_loss: 0.202
val_loss: 0.197
val_loss: 0.204
val_loss: 0.197
val_loss: 0.212
val_loss: 0.192
val_loss: 0.180
val_loss: 0.165
model saved.
val_loss: 0.199
val_loss: 0.229
val_loss: 0.189
val_loss: 0.202
val_loss: 0.161
model saved.
val_loss: 0.198
val_loss: 0.176
val_loss: 0.217
val_loss: 0.180
val_loss: 0.233
val_loss: 0.197
val_loss: 0.181
val_loss: 0.156
model saved.
val_loss: 0.177
val_loss: 0.211
val_loss: 0.157
val_loss: 0.170
val_loss: 0.196
val_loss: 0.170
val_loss: 0.162
val_loss: 0.164
val_loss: 0.168
val_loss: 0.191
val_loss: 0.177
val_loss: 0.181
val_loss: 0.203
val_loss: 0.172
val_loss: 0.154
model saved.
val_loss: 0.153
model saved.
val_loss: 0.168
val_loss: 0.200
val_loss: 0.159
val_loss: 0.172
val_loss: 0.151
model saved.
val_loss: 0.180
val_loss: 0.161
val_loss: 0.201
val_loss: 0.199
val_loss: 0.170
val_loss: 0.170
val_loss: 0.186
val_loss: 0.178
val_loss: 0.176
val_loss: 0.164
val_loss: 0.157
val_loss: 0.153
val_loss: 0.173
val_loss: 0.166
val_loss: 0.147
model saved.
val_loss: 0.164
val_loss: 0.166
val_loss: 0.167
val_loss: 0.163
val_loss: 0.156
val_loss: 0.171
val_loss: 0.161
val_loss: 0.172
val_loss: 0.182
val_loss: 0.182
val_loss: 0.148
val_loss: 0.184
val_loss: 0.185
val_loss: 0.162
val_loss: 0.161
val_loss: 0.194
val_loss: 0.159
val_loss: 0.152
val_loss: 0.166
val_loss: 0.158
val_loss: 0.153
val_loss: 0.159
val_loss: 0.162
val_loss: 0.150
val_loss: 0.171
val_loss: 0.179
